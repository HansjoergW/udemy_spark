{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp df_people_basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grundlagen von Sparks Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Autoreload funktioniert nicht bei XDebug\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession object at 0x0000027FAC53A8C8>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from udemy_spark.spark_core import *\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # Ausgabe der wichtigsten Session Informationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden von Dummy Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people = spark.read.json('people.json')\n",
    "df_apple  = spark.read.csv('appl_stock.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informationen von DF abfragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, name: string]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people.show()        # zeigt die Daten\n",
    "df_people.printSchema() # zeigt das Schema\n",
    "df_people.columns       # Attribut mit den Columnnames\n",
    "df_people.describe()    # Beschreibung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden von Daten\n",
    "### JSON\n",
    "#### typisiert lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (StructField,StringType,IntegerType,StructType)\n",
    "data_schema = [StructField(\"age\", IntegerType(), True),StructField(\"name\", StringType(), True)]\n",
    "final_struc = StructType(fields=data_schema)\n",
    "df_people = spark.read.json('people.json', schema=final_struc)\n",
    "df_people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhalt von DF anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people.head(2) # liefert eine Liste mit Row-Elementen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "+----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_people.show(2) # zeigt 2 Zeilen -> df_people.show() zeigt alles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zugriff auf Spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'age'>\n",
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "print(df_people['age'])\n",
    "print(type(df_people['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, name: string]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people.select('age')#.show() -> df_people.select -> returns a dataframe, not just a  column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, name: string]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people.select(['age','name']) #.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten filtern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like SQL-Where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Open|     Close|\n",
      "+----------+----------+\n",
      "|213.429998|214.009998|\n",
      "|214.599998|214.379993|\n",
      "|214.379993|210.969995|\n",
      "+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_apple.filter(\"Close<500\") # Die Filter koennen wie Where-Klauseln von SQL Statements definiert werden\n",
    "df_apple.filter(\"Close<500\").select(['Open','Close']).show(3)  # filter gibt wieder ein DF zurueck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Python / Pandas Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|\n",
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|               Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
      "|2010-01-28 00:00:00|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+------------------+----------+------------------+----------+---------+------------------+\n",
      "|               Date|              Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+----------+------------------+----------+---------+------------------+\n",
      "|2010-02-01 00:00:00|192.36999699999998|     196.0|191.29999899999999|194.729998|187469100|         25.229131|\n",
      "|2010-02-02 00:00:00|        195.909998|196.319994|193.37999299999998|195.859997|174585600|25.375532999999997|\n",
      "+-------------------+------------------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_apple.filter(df_apple['Close'] < 500).show(2) # standard Pandas Style\n",
    "df_apple.filter( (df_apple[\"Close\"] < 200) & (df_apple['Open'] > 200) ).show(2) # Achtung: die Klammern sind wichtig, anstelle von 'and' und 'or' & und | verwenden\n",
    "df_apple.filter( (df_apple[\"Close\"] < 200) & ~(df_apple['Open'] > 200) ).show(2) # ~ ist not Funktion: hier \"nicht groesser als 200\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rueckgabe als Python Objekte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "result = df_apple.filter(df_apple[\"Low\"] == 197.16).collect()\n",
    "print(type(result)) # Python Liste\n",
    "print(type(result[0])) # mit Spark Row Objekten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': datetime.datetime(2010, 1, 22, 0, 0), 'Open': 206.78000600000001, 'High': 207.499996, 'Low': 197.16, 'Close': 197.75, 'Volume': 220441900, 'Adj Close': 25.620401}\n",
      "2010-01-22 00:00:00 | 206.78000600000001 | 207.499996 | 197.16 | 197.75 | 220441900 | 25.620401 | "
     ]
    }
   ],
   "source": [
    "print(result[0].asDict()) # eine Row kann direkt als Dict zurueckgegeben werden\n",
    "for item in result[0]: # ueber die Spalten in einer Row kann auch direkt iteriert werden\n",
    "    print(item, end=' | ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation von DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neue Kolonnen erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people.withColumn('newage', df_people['age']).show()   # erzeugt ein neues DF mit einer neue Spalte basierend auf 'age'\n",
    "df_people.withColumn('newage', df_people['age']*2).show()   # erzeugt ein neues DF mit einer neue Spalte basierend auf 'age'\n",
    "df_people.withColumnRenamed('age','newage').show() # erzeugt neues DF mit umbenannter Spalte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registriere das Datenframe als voruebergehende SQL-Ansicht\n",
    "df_people.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_results = spark.sql(\"SELECT * FROM people\") # sql_results ist ein Dataframe\n",
    "sql_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (XPython)",
   "language": "python",
   "name": "xpython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
