{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp df_group_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gruppieren und aggregieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Autoreload funktioniert nicht bei XDebug\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1bec29d1b08>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from udemy_spark.spark_core import *\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # Ausgabe der wichtigsten Session Informationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales = spark.read.csv('data/sales_info.csv',inferSchema=True,header=True)\n",
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gruppieren und Aggregieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.groupBy(\"Company\") # gibt ein GroupedData objekt zur√ºck http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData\n",
    "df_sales.groupBy(\"Company\").mean().show() # weitere Methoden count, max, min, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weitere Methoden unter:  http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|Company|count(1)|\n",
      "+-------+--------+\n",
      "|   APPL|       4|\n",
      "|   GOOG|       3|\n",
      "|     FB|       2|\n",
      "|   MSFT|       3|\n",
      "+-------+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      12|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.groupBy(\"Company\").agg({\"*\":\"count\"}).show()\n",
    "df_sales.agg({\"*\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------+\n",
      "|Company|       avg(Sales)|count(1)|\n",
      "+-------+-----------------+--------+\n",
      "|   APPL|            370.0|       4|\n",
      "|   GOOG|            220.0|       3|\n",
      "|     FB|            610.0|       2|\n",
      "|   MSFT|322.3333333333333|       3|\n",
      "+-------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.groupBy(\"Company\").agg({\"Sales\":\"max\", \"Sales\":\"mean\",\"*\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionen\n",
    "Uebersicht http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Distinct Sales|\n",
      "+--------------+\n",
      "|            11|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.select(countDistinct(\"Sales\").alias(\"Distinct Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Sales)|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.select(avg(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genauigkeit formatieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|format_number(avg, 2)|\n",
      "+---------------------+\n",
      "|               360.58|\n",
      "+---------------------+\n",
      "\n",
      "+------+\n",
      "|   avg|\n",
      "+------+\n",
      "|360.58|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select = df_sales.select(avg(\"Sales\").alias(\"avg\"))\n",
    "df_select.select(format_number(\"avg\",2)).show()\n",
    "df_sales.select(format_number(avg(\"Sales\"),2).alias(\"avg\")).show() # oder gleich direkt in einer Zeile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order BY\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+------+-----+\n",
      "|Company|Person|Sales|\n",
      "+-------+------+-----+\n",
      "|     FB|  Carl|870.0|\n",
      "|   APPL|  Mike|750.0|\n",
      "+-------+------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+------+-----+\n",
      "|Company|Person|Sales|\n",
      "+-------+------+-----+\n",
      "|   APPL| Chris|350.0|\n",
      "|   MSFT|   Amy|124.0|\n",
      "+-------+------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+------+-----+\n",
      "|Company|Person|Sales|\n",
      "+-------+------+-----+\n",
      "|     FB|  Carl|870.0|\n",
      "|   APPL|  Mike|750.0|\n",
      "+-------+------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.sort(\"Sales\").show(2) # same as df_sales.orderBy(\"Sales\").show(2)\n",
    "df_sales.sort(\"Sales\", ascending=False).show(2)\n",
    "df_sales.sort([\"Person\",\"Sales\"],ascending=[True,False]).show(2) #\n",
    "df_sales.sort(df_sales.Sales).show(2)\n",
    "df_sales.sort(df_sales.Sales.desc()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
